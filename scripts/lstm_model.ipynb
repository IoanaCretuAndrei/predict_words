{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90cd878",
   "metadata": {},
   "source": [
    "### Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f484fa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ioana\\AppData\\Local\\Temp\\ipykernel_372\\3882199999.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pkg_resources\n",
    "import pickle\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37822b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "\n",
    "def load_doc (filename):\n",
    "    #open the file as read only\n",
    "    file=open(filename,'r')\n",
    "    #read all the text\n",
    "    text=file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def tokenize_twitter (sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "\n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    print('---------------------------')\n",
    "    print(\">>Starting cleaning process\")\n",
    "    print('---------------------------')\n",
    "    tokenized_sentences=[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        \n",
    "        #Convert to lowercase letters\n",
    "        sentence=cleanhtml(sentence)\n",
    "        sentence=_replace_urls(sentence)\n",
    "        sentence=remove_email(sentence)\n",
    "        sentence=re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "        sentence= sentence.lower()\n",
    "        \n",
    "    def cleanhtml(raw_html):\n",
    "        cleanr=re.compile('<.*?>')\n",
    "        cleantext=re.sub(cleanr,'',raw_html)\n",
    "        return cleantext\n",
    "    \n",
    "    def _replace_urls(data):\n",
    "        #Removing URLs with a regular expression\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        data=url_pattern.sub(r'', data)\n",
    "        return data\n",
    "    def remove_email(data):\n",
    "        #Remove Emails\n",
    "        data= re.sub('\\S*@\\S*\\s?','', data)\n",
    "        return data\n",
    "        \n",
    "    def misc (data):\n",
    "        #Remove new line characters\n",
    "        data=re.sub('\\s+', ' ', data)\n",
    "        #Remove distracting single quotes\n",
    "        data=re.sub(\"\\'\", \"\", data)\n",
    "        data=re.sub(\"ww+\", \"\", data)\n",
    "        #Removing roman-case\n",
    "        MAYBE_ROMAN = re.compile(r'\\b[MDCLXVI]+\\b)(\\.)?', re.I)\n",
    "        data=re.sub(MAYBE_ROMAN, \"\", data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552fca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def littleCleaning (sentences):\n",
    "    print('Starting cleaning process')\n",
    "    ret_list = []\n",
    "    for sentence in sentences: \n",
    "        words=sentence.split(\" \")\n",
    "        if len(words) > 5:\n",
    "            ret_list.append(sentence)\n",
    "        else:\n",
    "            continue\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350efa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdbd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengh of the corpus is:  1174387\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/republic.txt\"\n",
    "text = open(path).read().lower()\n",
    "print('Length of the corpus is: ', len(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de27d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of the republic, by plato\\n\\nthis ebook is for the use of anyone anywhere in the united states and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever',\n",
       " ' you may copy it, give it away or re-use it under the terms\\nof the project gutenberg license included with this ebook or online at\\nwww',\n",
       " 'gutenberg',\n",
       " 'org',\n",
       " ' if you are not located in the united states, you\\nwill have to check the laws of the country where you are located before\\nusing this ebook',\n",
       " '\\n\\ntitle: the republic\\n\\nauthor: plato\\n\\ntranslator: b',\n",
       " ' jowett\\n\\nrelease date: october, 1998 [ebook #1497]\\n[most recently updated: september 11, 2021]\\n\\nlanguage: english\\n\\n\\nproduced by: sue asscher and david widger\\n\\n*** start of the project gutenberg ebook the republic ***\\n\\n\\n\\n\\nthe republic\\n\\nby plato\\n\\ntranslated by benjamin jowett\\n\\nnote: see also “the republic” by plato, jowett, ebook #150\\n\\n\\ncontents\\n\\n introduction and analysis',\n",
       " '\\n the republic',\n",
       " '\\n persons of the dialogue',\n",
       " '\\n book i',\n",
       " '\\n book ii',\n",
       " '\\n book iii',\n",
       " '\\n book iv',\n",
       " '\\n book v',\n",
       " '\\n book vi',\n",
       " '\\n book vii',\n",
       " '\\n book viii',\n",
       " '\\n book ix',\n",
       " '\\n book x',\n",
       " '\\n\\n\\n\\n\\n introduction and analysis']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Converting data into lists\n",
    "data_list=text.split(\".\")\n",
    "data_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5d8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting normalization process...\n",
      "---------------------------\n",
      ">>Starting cleaning process\n",
      "---------------------------\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormalization process finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n\u001b[1;32m---> 10\u001b[0m pro_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mnormalization_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of sentences after normalization: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pro_sentences))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSome sample sentences: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mnormalization_pipeline\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalization_pipeline\u001b[39m(sentences):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting normalization process...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_twitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m littleCleaning(sentences)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormalization process finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mtokenize_twitter\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m tokenized_sentences\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     26\u001b[0m     \n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#Convert to lowercase letters\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     sentence\u001b[38;5;241m=\u001b[39mcleanhtml(sentence)\n\u001b[0;32m     29\u001b[0m     sentence\u001b[38;5;241m=\u001b[39m_replace_urls(sentence)\n",
      "File \u001b[1;32mc:\\Users\\ioana\\miniconda3\\envs\\my_env\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ioana\\miniconda3\\envs\\my_env\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "pro_sentences = []\n",
    "\n",
    "def normalization_pipeline(sentences):\n",
    "    print('Starting normalization process...')\n",
    "    sentences = tokenize_twitter(sentences)\n",
    "    sentences = littleCleaning(sentences)\n",
    "    print('Normalization process finished.')\n",
    "    return sentences\n",
    "\n",
    "pro_sentences = normalization_pipeline(data_list)\n",
    "print('Number of sentences after normalization: ', len(pro_sentences))\n",
    "print('Some sample sentences: ')\n",
    "for i in range(5):\n",
    "    print(pro_sentences[i])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
